{
 "metadata": {
  "name": "",
  "signature": "sha256:d767e95accc5ad6b077d3940ec18e36d4f4e7fba1649ebcbede1fd30b70a3fae"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Logistic Regression\n",
      "\n",
      "### 2nd December 2014 Neil D. Lawrence"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# download the software\n",
      "import urllib\n",
      "\n",
      "urllib.urlretrieve('https://github.com/sods/ods/archive/master.zip', 'master.zip')\n",
      "\n",
      "# unzip the software\n",
      "import zipfile\n",
      "zip = zipfile.ZipFile('./master.zip', 'r')\n",
      "for name in zip.namelist():\n",
      "    zip.extract(name, '.')\n",
      "\n",
      "# add the module location to the python path.    \n",
      "import sys\n",
      "sys.path.append(\"./ods-master/\") "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The [naive Bayes assumption](./week9.ipynb) allowed us to specify a class conditional density, $p(\\mathbf{x}_i|y_i, \\boldsymbol{\\theta})$, through assuming that the features were conditionally independent given the label. Combined with our assumption that the data points are conditionally independent given the parameters, $\\boldsymbol{\\theta}$, this allowed us to specify a joint density over the entire data set, $p(\\mathbf{y}, \\mathbf{X})$. We argued that modeling the joint density is a powerful approach because we can answer any particular question we have about the data through the sum rule and the product rule of probability. We can condition on the training data and query the value of an unseen test point. If we have missing data, then we can integrate over the missing point (marginalise) and obtain our best prediction despite the absence of some of the features for a point. However, it comes at the cost of a particular modeling assumption. Namely, to make modeling practical we assumed that the features were conditionally independent given the feature label. In other words, for any given point, if we know its class, then its features will be independent. This is a very strong assumption. For example, if we were classifying the sex of an individual given their height and weight, naive Bayes would assume that if we knew their sex, then the height and weight would be independent. This is clearly wrong, the dependence between height and weight is not dictated only by the sex of an individual, there is a natural correlation between them.\n",
      "\n",
      "Modeling the entire joint density allows us to deal with different questions, that we may not have envisaged at the model *design time*. It contrasts with the approach we took for regression where we specifically chose to model the conditional density for the target values, $\\mathbf{y}$, given the input locations, $\\mathbf{X}$. That density, $p(\\mathbf{y}|\\mathbf{X})$, effectively assumes that the question we'll be asked at *run time* is known. In particular, we expect to be asked about the value of the function, $y^*$, given a particular input location, $\\mathbf{x}^*$. We don't expect to be asked about the value of an input given a particular observation. That would require placing an additional prior over the input location for each point, $p(\\mathbf{x}_i)$. Of course, it's possible to conceive of a model like this, and indeed that is how we proceeded for [dimensionality reduction](./week8.ipynb). However, if we know we will always have all the inputs at run time, it may make sense to *directly* model the conditional density, $p(\\mathbf{y}|\\mathbf{X})$. \n",
      "\n",
      "## Logistic Regression\n",
      "\n",
      "A logistic regression is an approach to classification which extends the linear basis function models we've already explored. Rather than modeling the output of the function directly the assumption is that we model the *log-odds* with the basis functions.\n",
      "\n",
      "The [odds](http://en.wikipedia.org/wiki/Odds) are defined as the ratio of the probability of a positive outcome, to the probability of a negative outcome. If the probability of a positive outcome is denoted by $\\pi$, then the odds are computed as $\\frac{\\pi}{1-\\pi}$. Odds are widely used by [bookmakers](http://en.wikipedia.org/wiki/Bookmaker) in gambling, although a bookmakers odds won't normalise: i.e. if you look at the equivalent probabilities, and sum over the probability of all outcomes the bookmakers are considering, then you won't get one. This is how the bookmaker makes a profit. \n",
      "\n",
      "Because a probability is always between zero and one, the odds are always between $0$ and $\\infty$. If the positive outcome is unlikely the odds are close to zero, if it is very likely then the odds become close to infinite. Taking the logarithm of the odds maps the odds from the positive half space to being across the entire real line. Odds that were between 0 and 1 (where the negative outcome was more likely) are mapped to the range between $-\\infty$ and $0$. Odds that are greater than 1 are mapped to the range between $0$ and $\\infty$. Considering the log odds therefore takes a number between 0 and 1 (the probability of positive outcome) and maps it to the entire real line. The function that does this is known as the [logit function](http://en.wikipedia.org/wiki/Logit),\n",
      "$\n",
      "g^{-1}(p_i) = \\log\\frac{p_i}{1-p_i}\n",
      "$\n",
      "This function is known as a *link function*.\n",
      "\n",
      "For a standard regression we take,\n",
      "$$\n",
      "f(x) = \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}),\n",
      "$$\n",
      "if we want to perform classification we perform a logistic regression. \n",
      "$$\n",
      "\\log \\frac{\\pi}{(1-\\pi)} = \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})\n",
      "$$\n",
      "where the odds ratio between the positive class and the negative class is given by\n",
      "$$\n",
      "\\frac{\\pi}{(1-\\pi)}\n",
      "$$\n",
      "The odds can never be negative, but can take any value from 0 to $\\infty$. We have defined the link function as taking the form $g^{-1}(\\cdot)$ implying that the inverse link function is given by $g(\\cdot)$. Since we have defined,\n",
      "$$\n",
      "g^{-1}(\\pi) = \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})\n",
      "$$\n",
      "we can write $\\pi$ in terms of the *inverse link* function, $g(\\cdot)$ as \n",
      "$$\n",
      "\\pi = g(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})).\n",
      "$$\n",
      "This inverse of the link function is known as the [logistic](http://en.wikipedia.org/wiki/Logistic_function) (thus the name logistic regression) or sometimes it is called the sigmoid function. For a particular value of the input to the link function, $f_i = \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_i)$ we can plot the value of the link function as below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "f = np.linspace(-6, 6, 100)\n",
      "g = 1/(1+np.exp(-f))\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "plt.plot(f, g, 'r-')\n",
      "plt.title('Logistic Function')\n",
      "plt.xlabel('$f_i$')\n",
      "plt.ylabel('$g_i$')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "<matplotlib.text.Text at 0x10b558e90>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEeCAYAAABonHmPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiIiIiIiIiIiIiIiCTL/wM4z1Gbr2gjkwAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10be2b250>"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The function has this characeristic 's'-shape (from where the term sigmoid, as in sigma, comes from). It also takes the input from the entire real line and 'squashes' it into an output that is between zero and one. For this reason it is sometimes also called a 'squashing function'. \n",
      "\n",
      "By replacing the inverse link with the sigmoid we can write $\\pi$ as a function of the input and the parameter vector as, \n",
      "$$\n",
      "\\pi(\\mathbf{x},\\mathbf{w}) = \\frac{1}{1+ \\exp\\left(-\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})\\right)}.\n",
      "$$\n",
      "\n",
      "The process for logistic regression is as follows. Compute the output of a standard linear basis function composition ($\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})$, as we did for linear regression) and then apply the inverse link function, $g(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}))$. In logistic regression this involves *squashing* it with the logistic (or sigmoid) function. Use this value, which now has an interpretation as a *probability* in a Bernoulli distribution to form the likelihood. Then we can assume conditional independence of each data point given the parameters and develop a likelihod for the entire data set. \n",
      "\n",
      "As we discussed last time, the Bernoulli likelihood is of the form,\n",
      "$$\n",
      "P(y_i|\\mathbf{w}, \\mathbf{x}) = \\pi_i^{y_i} (1-\\pi_i)^{1-y_i}\n",
      "$$\n",
      "which we can think of as clever trick for mathematically switching between two probabilities if we were to write it as code it would be better described as\n",
      "```python\n",
      "def bernoulli(x, y, pi):\n",
      "    if y == 1:\n",
      "        return pi(x)\n",
      "    else:\n",
      "        return 1-pi(x)\n",
      "```\n",
      "but writing it mathematically makes it easier to write our objective function within a single mathematical equation. \n",
      "\n",
      "## Maximum Likelihood\n",
      "\n",
      "To obtain the parameters of the model, we need to maximize the likelihood, or minimize the objective function, normally taken to be the negative log likelihood. With a data conditional independence assumption the likelihood has the form,\n",
      "$$\n",
      "P(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}) = \\prod_{i=1}^n P(y_i|\\mathbf{w}, \\mathbf{x}_i). \n",
      "$$\n",
      "which can be written as a log likelihood in the form\n",
      "$$\n",
      "\\log P(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}) = \\sum_{i=1}^n \\log P(y_i|\\mathbf{w}, \\mathbf{x}_i) = \\sum_{i=1}^n y_i \\log \\pi_i + \\sum_{i=1}^n (1-y_i)\\log (1-\\pi_i)\n",
      "$$\n",
      "and if we take the probability of positive outcome for the $i$th data point to be given by\n",
      "$$\n",
      "\\pi_i = g\\left(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_i)\\right),\n",
      "$$\n",
      "where $g(\\cdot)$ is the *inverse* link function, then this leads to an objective function of the form,\n",
      "$$\n",
      "E(\\mathbf{w}) = -  \\sum_{i=1}^n y_i \\log g\\left(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_i)\\right) - \\sum_{i=1}^n(1-y_i)\\log \\left(1-g\\left(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}_i)\\right)\\right).\n",
      "$$\n",
      "\n",
      "As normal, we would like to minimize this objective. This can be done by differentiating with respect to the parameters of our prediction function, $\\pi(\\mathbf{x};\\mathbf{w})$, for optimisation. The gradient of the likelihood with respect to $\\pi(\\mathbf{x};\\mathbf{w})$ is of the form,\n",
      "$$\n",
      "\\frac{\\text{d}E(\\mathbf{w}}{\\text{d}\\mathbf{w}} = -\\sum_{i=1}^n \\frac{y_i}{g\\left(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})\\right)}\\frac{\\text{d}g(f_i)}{\\text{d}f_i} \\boldsymbol{\\phi(\\mathbf{x}_i)} +  \\sum_{i=1}^n \\frac{1-y_i}{1-g\\left(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})\\right)}\\frac{\\text{d}g(f_i)}{\\text{d}f_i} \\boldsymbol{\\phi(\\mathbf{x}_i)}\n",
      "$$\n",
      "where we used the chain rule to develop the derivative in terms of $\\frac{\\text{d}g(f_i)}{\\text{d}f_i}$, which is the gradient of the inverse link function (in our case the gradient of the sigmoid function).\n",
      "\n",
      "So the objective function now depends on the gradient of the inverse link function, as well as the likelihood depends on the gradient of the inverse link function, as well as the gradient of the log likelihood, and naturally the gradient of the argument of the inverse link function with respect to the parameters, which is simply $\\boldsymbol{\\phi}(\\mathbf{x}_i)$.\n",
      "\n",
      "The only missing term is the gradient of the inverse link function. For the sigmoid squashing function we have,\n",
      "\\begin{align*}\n",
      "g(f_i) &= \\frac{1}{1+\\exp(-f_i)}\\\\\n",
      "&=(1+\\exp(-f_i))^{-1}\n",
      "\\end{align*}\n",
      "and the gradient can be computed as\n",
      "\\begin{align*}\n",
      "\\frac{\\text{d}g(f_i)}{\\text{d} f_i} & = \\exp(-f_i)(1+\\exp(-f_i))^{-2}\\\\\n",
      "& = \\frac{1}{1+\\exp(-f_i)} \\frac{\\exp(-f_i)}{1+\\exp(-f_i)} \\\\\n",
      "& = g(f_i) (1-g(f_i))\n",
      "\\end{align*}\n",
      "so the full gradient can be written down as\n",
      "$$\n",
      "\\frac{\\text{d}E(\\mathbf{w})}{\\text{d}\\mathbf{w}} = -\\sum_{i=1}^n y_i\\left(1-g\\left(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})\\right)\\right) \\boldsymbol{\\phi(\\mathbf{x}_i)} +  \\sum_{i=1}^n (1-y_i)\\left(g\\left(\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})\\right)\\right) \\boldsymbol{\\phi(\\mathbf{x}_i)}.\n",
      "$$\n",
      "\n",
      "## Optimization of the Function\n",
      "\n",
      "Reorganizing the gradient to find a stationary point of the function with respect to the parameters $\\mathbf{w}$ turns out to be impossible. Optimization has to proceed by *numerical methods*. Options include the multidimensional variant of [Newton's method](http://en.wikipedia.org/wiki/Newton%27s_method) or [gradient based optimization methods](http://en.wikipedia.org/wiki/Gradient_method) like we used for optimizing matrix factorization for the movie recommender system. We recall from matrix factorization that, for large data, *stochastic gradient descent* or the Robbins Munroe optimization procedure worked best for function minimization. \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pods\n",
      "pods.datasets.movie_body_count_r_classify()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed',)).History will not be written to the database.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/neill/sods/ods/pods/datasets.py:1123: SettingWithCopyWarning: \n",
        "A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_indexer,col_indexer] = value instead\n",
        "\n",
        "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
        "  X[genre] = np.zeros(X.shape[0])\n"
       ]
      },
      {
       "ename": "KeyError",
       "evalue": "'the label [Biography] is not in the [index]'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-84dd8763b4a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmovie_body_count_r_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/Users/neill/sods/ods/pods/datasets.py\u001b[0m in \u001b[0;36mmovie_body_count_r_classify\u001b[0;34m(data_set)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mgenre\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgenre\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgenre\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m     return data_details_return({'X': X, 'Y': Y, 'info' : \"Data set of movies and body count for movies scraped from www.MovieBodyCounts.com created by Simon Garnier and Randy Olson for exploring differences between Python and R. In this variant we aim to classify whether the film is rated R or not depending on the genre, the years and the body count.\",\n\u001b[1;32m   1126\u001b[0m                                 }, data_set)\n",
        "\u001b[0;32m/Users/neill/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/neill/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/neill/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_has_valid_type\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m                 \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/neill/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36merror\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1284\u001b[0m                         \"cannot use label indexing with a null key\")\n\u001b[1;32m   1285\u001b[0m                 raise KeyError(\"the label [%s] is not in the [%s]\" %\n\u001b[0;32m-> 1286\u001b[0;31m                                (key, self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyError\u001b[0m: 'the label [Biography] is not in the [index]'"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}