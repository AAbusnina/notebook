{
 "metadata": {
  "name": "",
  "signature": "sha256:c7ea01c84449b43bbb935763265a6c349f7bcab7f8ed85f877c21413de4b6288"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Logistic Regression\n",
      "\n",
      "### 2nd December 2014 Neil D. Lawrence\n",
      "\n",
      "A logistic regression is an approach to classification which extends the linear basis function models we've already explored. Rather than modeling the output of the function directly the assumption is that we model the log-odds with the basis functions.\n",
      "\n",
      "In standard regression we take,\n",
      "$$\n",
      "f(x) = \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}).\n",
      "$$\n",
      "$$\n",
      "\\log \\frac{\\pi}{(1-\\pi)} = \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x})\n",
      "$$\n",
      "where the odds ratio between the positive class and the negative class is given by\n",
      "$$\n",
      "\\frac{\\pi}{(1-\\pi)}\n",
      "$$\n",
      "The odds can never be negative, but can take any value from 0 to $\\infty$. We can therefore rewrite the function as the \n",
      "$$\n",
      "\\pi(x) = \\frac{1}{1- \\exp(-\\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{x}))}\n",
      "$$\n",
      "This probability is used within a Bernoulli likelihood so that,\n",
      "$$\n",
      "p(y_i|\\mathbf{w}, \\mathbf{x}) = \\pi(\\mathbf{x})^y_i (1-\\pi(\\mathbf{x}))^{1-y_i}\n",
      "$$\n",
      "the Bernoulli distribution is a clever trick for mathematically switching between two probabilities if we were to write it as code it would be better described as\n",
      "```python\n",
      "def bernoulli(x, pi):\n",
      "    if y_i == 1:\n",
      "        return pi(x)\n",
      "    else:\n",
      "        return 1-pi(x)\n",
      "```\n",
      "but writing it mathematically makes it easier to write the log likelihood,\n",
      "$$\n",
      "\\log p(y_i|\\mathbf{w}, \\mathbf{x}) = y_i \\log \\pi(\\mathbf{x}) (1-y_i)\\log (1-\\pi(\\mathbf{x}))\n",
      "$$\n",
      "which is what we need to differentiate with respect to the parameters of $\\pi(x)$ for optimisation.\n",
      "\n",
      "The gradient of the likelihood with respect to $\\pi(\\mathbf{x})$ is of the form\n",
      "$$\n",
      "\\frac{\\text{d}\\log p(y_i|\\mathbf{w}, \\mathbf{x})}{\\text{d}\\mathbf{w}} = (y_i(1-\\pi(\\mathbf{x})) + (1-y_i)\\pi(\\mathbf{x}))\\boldsymbol{\\phi_i}\n",
      "$$\n",
      "and taking the logarithm gives us the loga\n",
      "is given by \n",
      "In a logistic discriminiant"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}