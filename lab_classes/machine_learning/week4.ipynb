{
 "metadata": {
  "name": "",
  "signature": "sha256:81a0aebfb6082ec96bb93f5134a146449337ba34b0138d9b8a1e78261bf63970"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Basis Functions\n",
      "\n",
      "### 21st October 2014 Neil Lawrence\n",
      "\n",
      "Multivariate linear regression allows us to build models that take many features into account when making our prediction. In this session we are going to introduce *basis functions*. The term seems complicted, but they are actually based on rather a simple idea. If we are doing a multivariate linear regression, we get extra features that *might* help us predict our required response varible (or target value), $y$. But what if we only have one input value? We can actually artificially generate more input values with basis functions.\n",
      "\n",
      "Here's the idea, instead of working directly on the original input space, $\\mathbf{x}$, we build models in a new space, $\\boldsymbol{\\phi}(\\mathbf{x})$ where $\\boldsymbol{\\phi}(\\cdot)$ is a *vector valued* function that is defined on the space $\\mathbf{x}$. Remember, that a vector valued function is just a vector with functions in it instead of values. Here's an example for a one dimensional input space, $x$, being projected to a *quadratic* basis,\n",
      "$$\n",
      "\\boldsymbol{\\phi}(x) = \\begin{bmatrix} 1\\\\ x \\\\ x^2\\end{bmatrix}.\n",
      "$$\n",
      "We like to make use of *design* matrices for our data. Design matrices, as you will recall, involve placing the data points into rows of the matrix and data features into the columns of the matrix. By convention, we are referincing a vector with a bold lower case letter, and a matrix with a bold upper case letter. The design matrix is therefore given by\n",
      "$$\n",
      "\\boldsymbol{\\Phi} = \\begin{bmatrix} 1 & \\mathbf{x} & \\mathbf{x}^2\\end{bmatrix}\n",
      "$$\n",
      "where \n",
      "which we can plot as follows"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.linspace(-1, 1, 100)\n",
      "phi_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Basis Functions: Quadratic Fit to Marathon Data\n",
      "\n",
      "Now we will fit a quadratic model using basis functions. Given everything we've learnt above, this is now quite easy to do. Firstly, we need to create a design matrix that contains the quadratic basis, \n",
      "\n",
      "$$\\mathbf{\\Phi} = \\left[ \\mathbf{1} \\quad \\mathbf{x} \\quad \\mathbf{x}^2\\right]$$\n",
      "\n",
      "where this notation means that each column of $\\mathbf{\\Phi}$ is derived from the entire set of input years.\n",
      "\n",
      "Phi = np.hstack([np.ones(x.shape), x, x**2])\n",
      "\n",
      "Now we can solve this system for $\\mathbf{w}$ just as we did for the linear case, so we have,\n",
      "\n",
      "w = np.linalg.solve(np.dot(Phi.T, Phi), np.dot(Phi.T, y))\n",
      "print(w)\n",
      "\n",
      "We can plot the solution in two different ways, either we take\n",
      "\n",
      "f_test = w[2]*x_test**2 + w[1]*x_test + w[0]\n",
      "plt.plot(x_test, f_test, 'b-')\n",
      "plt.plot(x, y, 'rx')"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Or we can do the matrix form of this equation which first involves creating a design matrix for the test points,\n",
      "\n",
      "\n",
      "Phi_test = np.hstack((np.ones_like(x_test), x_test, x_test**2))\n",
      "\n",
      "and then computing the value of the function using a matrix multiply\n",
      "\n",
      "f_test = np.dot(Phi_test,w)\n",
      "plt.plot(x_test, f_test, 'b-')\n",
      "plt.plot(x, y, 'rx')\n",
      "\n",
      "Note the values of the coefficient $w_2$ in particular. It is relatively small, because it is multiplying a large number (square of 2000 is 4 million). This need to use small coefficients becomes worse as we increase the order of the fit. As an exercise for this week, try fitting higher order polynomials to the data. See what happens as you increase the polynomial order."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "http://en.wikipedia.org/wiki/Invertible_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}