{
 "metadata": {
  "name": "",
  "signature": "sha256:b1b933af5f37fa94ba1ff9e9772eb92f11f7cb0518e90d0cd4cf8c61ff8d2e43"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Model checking: Training, Testing and Validation\n",
      "\n",
      "### 4th November 2014 Neil D. Lawrence"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# download the software\n",
      "import urllib\n",
      "\n",
      "urllib.urlretrieve('https://github.com/sods/ods/archive/master.zip', 'master.zip')\n",
      "\n",
      "# unzip the software\n",
      "import zipfile\n",
      "zip = zipfile.ZipFile('./master.zip', 'r')\n",
      "for name in zip.namelist():\n",
      "    zip.extract(name, '.')\n",
      "\n",
      "# add the module location to the python path.    \n",
      "import sys\n",
      "sys.path.append(\"./ods-master/\") "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Introduction\n",
      "\n",
      "Minimising an objective function only tells us about the performance of a model on the training data, it doesn't inform us how it is likely to work on future data. In this lab we will explore techniques for model selection that make use of validation data. Data that isn't seen by the model in the learning (or fitting) phase, but is used to *validate* our choice of model from amoungst the different designs we have selected.\n",
      "\n",
      "In machine learning, we are looking to minimise the value of our objective function $E$ with respect to its parameters $\\mathbf{w}$. We do this by considering our training data. We minimize the value of the objective function as it's observed at each training point. However we are really interested in how the model will perform on future data. For evaluating that we choose to *hold out* a portion of the data for evaluating the quality of the model.\n",
      "\n",
      "We will review the different methods of model selection on the Olympics marathon data. Firstly we import the olympics data. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pods\n",
      "data = pods.datasets.olympic_marathon_men()\n",
      "x = data['X']\n",
      "y = data['Y']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can plot them to check that they've loaded in correctly.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import pylab as plt\n",
      "plt.plot(x, y, 'rx')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Hold Out Validation\n",
      "\n",
      "The first thing we'll do is fit a standard linear model to the data. We recall from previous lectures and lab classes that to do this we need to solve the system\n",
      "$$\n",
      "\\boldsymbol{\\Phi}^\\top \\boldsymbol{\\Phi} \\mathbf{w} = \\boldsymbol{\\Phi}^\\top \\mathbf{y}\n",
      "$$\n",
      "for $\\mathbf{w}$  and use the resulting vector to make predictions at the training points and test points,\n",
      "$$\n",
      "\\mathbf{f} = \\boldsymbol{\\Phi} \\mathbf{w}.\n",
      "$$\n",
      "The prediction function can be used to compute the objective function,\n",
      "$$\n",
      "E(\\mathbf{w}) = \\sum_{i}^n (y_i - \\mathbf{w}^\\top\\phi(\\mathbf{x}_i))^2\n",
      "$$\n",
      "by substituting in the prediction in vector form we have\n",
      "$$\n",
      "E(\\mathbf{w}) =  (\\mathbf{y} - \\mathbf{f})^\\top(\\mathbf{y} - \\mathbf{f})\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Assignment Question 1\n",
      "\n",
      "In this question you will construct some flexible general code for fitting linear models.\n",
      "\n",
      "Create a python function that computes $\\boldsymbol{\\Phi}$ for the linear basis,\n",
      "$$\\boldsymbol{\\Phi} = \\begin{bmatrix} \\mathbf{x} & \\mathbf{1}\\end{bmatrix}$$\n",
      "Name your function `linear`. `Phi` should be in the form of a *design matrix* and `x` should be in the form of a `numpy` two dimensional array with $n$ rows and 1 column\n",
      "Calls to your function should be in the following form:\n",
      "\n",
      "```python\n",
      "Phi = linear(x)\n",
      "```\n",
      "\n",
      "Create a python function that accepts, as arguments, a python function that defines a basis (like the one you've just created called `linear`) as well as a set of inputs and a vector of parameters. Your new python function should return a prediction. Name your function `prediction`. The return value `f` should be a two dimensional `numpy` array with $n$ rows and $1$ column, where $n$ is the number of data points. Calls to your function should be in the following form:\n",
      "\n",
      "```python\n",
      "f = prediction(w, x, linear)\n",
      "```\n",
      "\n",
      "Create a python function that computes the sum of squares objective function (or error function). It should accept your input data (or covariates) and target data (or response variables) and your parameter vector `w` as arguments. It should also accept a python function that represents the basis. Calls to your function should be in the following form:\n",
      "\n",
      "```python\n",
      "e = objective(w, x, y, linear)\n",
      "```\n",
      "\n",
      "Create a function that solves the linear system for the set of parameters that minimizes the sum of squares objective. It should accept input data, target data and a python function for the basis as the inputs. Calls to your function should be in the following form:\n",
      "\n",
      "```python\n",
      "w = fit(x, y, linear)\n",
      "```\n",
      "\n",
      "Fit a linear model to the olympic data using these functions and plot the resulting prediction between 1890 and 2020. Set the title of the plot to be the error of the fit on the *training data*.\n",
      "\n",
      "*15 marks*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#### Question 1 Answer Code\n",
      "# Write code for you answer to this question in this box\n",
      "# Do not delete these comments, otherwise you will get zero for this answer.\n",
      "# Make sure your code has run and the answer is correct *before* submitting your notebook for marking."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Polynomial Fit: Training Error\n",
      "\n",
      "The next thing we'll do is consider a quadratic fit. We will compute the training error for the two fits.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Assignment Question 2\n",
      "\n",
      "In this question we extend the code above to a non-linear basis (a quadratic function).\n",
      "\n",
      "Start by creating a python-function called `quadratic`. It should compute the quadratic basis.\n",
      "$$\\boldsymbol{\\Phi} = \\begin{bmatrix} \\mathbf{1} & \\mathbf{x} & \\mathbf{x}^2\\end{bmatrix}$$\n",
      "It should be called in the following form:\n",
      "```python\n",
      "Phi = quadratic(x)\n",
      "```\n",
      "Use this to compute the quadratic fit for the model, again plotting the result titled by the error.\n",
      "\n",
      "*10 marks*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#### Question 2 Answer Code\n",
      "# Write code for you answer to this question in this box\n",
      "# Do not delete these comments, otherwise you will get zero for this answer.\n",
      "# Make sure your code has run and the answer is correct *before* submitting your notebook for marking."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Hold Out Data\n",
      "\n",
      "You have a conclusion as to which model fits best under the training error, but how do the two models perform in terms of validation? In this section we consider *hold out* validation. In hold out validation we remove a portion of the training data for *validating* the model on. The remaining data is used for fitting the model (training). Because this is a time series prediction, it makes sense for us to hold out data at the end of the time series. This means that we are validating on future predictions. We will hold out data from after 1980 and fit the model to the data before 1980. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# select indices of data to 'hold out'\n",
      "indices_hold_out = np.flatnonzero(x>1980)\n",
      "\n",
      "# Create a training set\n",
      "x_train = np.delete(x, indices_hold_out, axis=0)\n",
      "y_train = np.delete(y, indices_hold_out, axis=0)\n",
      "\n",
      "# Create a hold out set\n",
      "x_valid = np.take(x, indices_hold_out, axis=0)\n",
      "y_valid = np.take(y, indices_hold_out, axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Assignment Question 3\n",
      "\n",
      "For both the linear and quadratic models, fit the model to the data up until 1980 and then compute the error on the held out data (from 1980 onwards). Which model performs better on the validation data?\n",
      "\n",
      "*10 marks*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#### Question 3 Answer Code\n",
      "# Write code for you answer to this question in this box\n",
      "# Do not delete these comments, otherwise you will get zero for this answer.\n",
      "# Make sure your code has run and the answer is correct *before* submitting your notebook for marking."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Richer Basis Set\n",
      "\n",
      "Now we have an approach for deciding which model to retain, we can consider the entire family of polynomial bases, with arbitrary degrees.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Assignment Question 4\n",
      "\n",
      "Now we are going to build a more sophisticated form of basis function, one that can accept arguments to its inputs (similar to those we used in [this lab](./week4.ipynb). Here we will start with a polynomial basis.\n",
      "\n",
      "```python\n",
      "def polynomial(x, degree, loc, scale):\n",
      "    degrees = np.arange(degree)\n",
      "    return ((x-loc)/scale)**degrees\n",
      "```\n",
      "The basis as we've defined it has three arguments as well as the input. The degree of the polynomial, the scale of the polynomial and the offset. These arguments need to be passed to the basis functions whenever they are called. Modify your code to pass these additional arguments to the python function for creating the basis. Do this for each of your functions `predict`, `fit` and `objective`. You will find `*args` (or `**kwargs`) useful.\n",
      "\n",
      "Write code that tries to fit different models to the data with polynomial basis. Use a maximum degree for your basis from 0 to 17. For each polynomial store the *hold out validation error* and the *training error*. When you have finished the computation plot the hold out error for your models and the training error for your p. When computing your polynomial basis use `offset=1956.` and `scale=120.` to ensure that the data is mapped (roughly) to the -1, 1 range.\n",
      "\n",
      "Which polynomial has the minimum training error? Which polynomial has the minimum validation error?\n",
      "\n",
      "*25 marks*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#### Question 4 Answer Code\n",
      "# Write code for you answer to this question in this box\n",
      "# Do not delete these comments, otherwise you will get zero for this answer.\n",
      "# Make sure your code has run and the answer is correct *before* submitting your notebook for marking."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Leave One Out Validation\n",
      "\n",
      "Hold out validation uses a portion of the data to hold out and a portion of the data to train on. There is always a compromise between how much data to hold out and how much data to train on. The more data you hold out, the better the estimate of your performance at 'run-time' (when the model is used to make predictions in real applications). However, by holding out more data, you leave less data to train on, so you have a better validation, but a poorer quality model fit than you could have had if you'd used all the data for training. Leave one out cross validation leaves as much data in the training phase as possible: you only take *one point* out for your validation set. However, if you do this for hold-out validation, then the quality of your validation error is very poor because you are testing the model quality on one point only. In *cross validation* the approach is to improve this estimate by doing more than one model fit. In *leave one out cross validation* you fit $n$ different models, where $n$ is the number of your data. For each model fit you take out one data point, and train the model on the remaining $n-1$ data points. You validate the model on the data point you've held out, but you do this $n$ times, once for each different model. You then take the *average* of all the $n$ badly estimated hold out validation errors. The average of this estimate is a good estimate of performance of those models on the test data. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Assignment Question 5\n",
      "\n",
      "Write code that computes the *leave one out* validation error for the olympic data and the polynomial basis. Use the functions you have created above: `objective`, `fit`, `polynomial`. Compute the *leave-one-out* cross validation error for basis functions containing a maximum degree from 0 to 17.\n",
      "\n",
      "*20 marks*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#### Question 5 Answer Code\n",
      "# Write code for you answer to this question in this box\n",
      "# Do not delete these comments, otherwise you will get zero for this answer.\n",
      "# Make sure your code has run and the answer is correct *before* submitting your notebook for marking."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## $k$-fold Cross Validation\n",
      "\n",
      "Leave one out cross validation produces a very good estimate of the performance at test time, and is particularly useful if you don't have a lot of data. In these cases you need to make as much use of your data for model fitting as possible, and having a large hold out data set (to validate model performance) can have a significant effect on the size of the data set you have to fit your model, and correspondingly, the complexity of the model you can fit. However, leave one out cross validation involves fitting $n$ models, where $n$ is your number of training data. For the olympics example, this is only 27 model fits, but in practice many data sets consist thousands or millions of data points, and fitting many millions of models for estimating validation error isn't really practical. One option is to return to *hold out* validation, but another approach is to perform $k$-fold cross validation. In $k$-fold cross validation you split your data into $k$ parts. Then you use $k-1$ of those parts for training, and hold out one part for validation. Just like we did for the hold out validation above. In *cross* validation, however, you repeat this process. You swap the part of the data you just used for validation back in to the training set and select another part for validation. You then fit the model to the new training data and validate on the portion of data you've just extracted. Each split of training/validation data is called a *fold* and since you do this process $k$ times, the procedure is known as $k$-fold cross validation. The term *cross* refers to the fact that you cross over your validation portion back into the training data every time you perform a fold.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Assignment Question 6\n",
      "\n",
      "Perform $k$-fold cross validation on the olympic data with your polynomial basis. Use $k$ set to 5 (e.g. five fold cross validation). Do the different forms of validation select different models? \n",
      "\n",
      "*20 marks*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Note*: The data doesn't divide into 5 equal size partitions for the five fold cross validation error. Don't worry about this too much. Two of the partitions will have an extra data point. You might find `np.random.randperm?` useful."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#### Question 6 Answer Code\n",
      "# Write code for you answer to this question in this box\n",
      "# Do not delete these comments, otherwise you will get zero for this answer.\n",
      "# Make sure your code has run and the answer is correct *before* submitting your notebook for marking."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}